---
layout: post
title: "Celery"
date: 2021-11-09
categories: TIL Celery Redis
---

![](https://raw.githubusercontent.com/Action2theFuture/Action2theFuture.github.io/main/_posts/Images/celery.png)
![](https://raw.githubusercontent.com/Action2theFuture/Action2theFuture.github.io/main/_posts/Images/celery_process.jpeg)

Celery, Redis 비동기 작업 큐(Queue)

현재 프로젝트 상황에서 업로드되는 과정에서 소요되는 시간이 사용자입장에서는 불편하여  
비동기 작업으로 따로 서버를 구축을 하여서 이용하면 어떨까해서 Celery에 대해 조사를 해보았다

Celery는 Worker와 Broker로 이루어져 있다

## celery config

```python
CELERY_BROKER_URL = 'redis://localhost:6379/0'
BACKEND_URL = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'

from __future__ import absolute_import, unicode_literals
from celery import Celery
from settings import CELERY_BROKER_URL, BACKEND_URL

app = Celery("celery_app",broker=CELERY_BROKER_URL, backend=BACKEND_URL)
```

### app.config_from_object 활용하기

*celeryconfig.py*
```python
broker_url = 'redis://localhost:6379'
result_backend = 'redis://localhost:6379'
accept_content = ['pickle','json','application/x-python-serialize']
task_serializer = 'pickle'
result_serializer = 'pickle'
timezone = 'Asia/Seoul'
```

*celery_app/worker.py*
```python
from __future__ import absolute_import, unicode_literals
from celery import Celery

app = Celery('celery_app',include=['celery_app.tasks'])

app.config_from_object('celery_app.celeryconfig')
```

## Task
*celery_app/tasks.py*
```python
from celery_app.worker import app as celery_app

@celery_app.task
def add(x,y):
    return x+y
```

[Celery Task](https://docs.celeryproject.org/en/stable/userguide/tasks.html)

*※ Task Process 전달*
- Task function에 Process 전달을 하려면 해당 객체가 Celery 설정된 데이터형식에 직렬화가 가능해야한다
    - [Json](https://www.json.org/json-en.html) : JSON의 주요 단점은 문자열, 유니코드, 부동 소수점, 부울, 사전 및 목록과 같은 데이터 유형으로 제한된다는 것입니다. 
    소수점과 날짜가 특히 누락되었습니다.  
    바이너리 데이터는 Base64 인코딩을 사용하여 전송되며, 기본 바이너리 유형이 지원되는 인코딩 형식에 비해 전송되는 데이터의 크기가 34% 증가합니다.  
    그러나 데이터가 위의 제약 조건에 적합하고 언어 간 지원이 필요한 경우 JSON의 기본 설정이 가장 좋은 선택일 것입니다.  
    - [pickle](https://docs.python.org/dev/library/pickle.html#module-pickle) : 내장된 모든 Python 데이터 유형(클래스 인스턴스 제외)
     바이너리 파일을 보낼 때 더 작은 메시지, JSON 처리에 비해 약간의 속도 향상을 지원합니다
    - [yaml](http://yaml.org/) : 더 많은 데이터 유형(날짜, 재귀 참조 등 포함)을 기본적으로 지원합니다    
    AML용 Python 라이브러리는 JSON용 라이브러리보다 약간 느립니다.  
    보다 표현적인 데이터 유형 집합이 필요하고 언어 간 호환성을 유지해야 하는 경우 YAML이 위의 것보다 더 적합할 수 있습니다.
    - [msgpack](http://msgpack.org/) :  msgpack은 JSON에 가까운 바이너리 직렬화 형식입니다.

-

## Celery 동작
*main.py*
```python
from celery_app.tasks import add

add.apply_async(args=[1,2], link=None, link_error=None, ....)
# 기능을 추가할 수 있는 콜백함수를 인자값으로 넣을 수 있다
```


Celery 실행 명령어
`celery -A {file name} worker -I info`

celery-beat : 주기적으로 특정 프로세스를 동작시키기 위한 패키지

-A: application ( = -app )  => 확장자를 제외하고 입력함. 만약 tasks.py 이면 tasks를 입력

worker 는 기본 명령어이며 worker 서버를 구동하라는 뜻이다.


## schedule

```python
from celery.schedules import crontab
 
@app.task(bind=True)
def debug_task(self):
    print('Request: {0!r}'.format(self.request))
 
app.conf.beat_schedule = {
    'add-every-minute-contrab': {
        'task': 'multiply_two_numbers',
        'schedule': crontab(),  # 1분마다
        'args': (16, 16),
    },
    'add-every-5-seconds': {
        'task': 'multiply_two_numbers',
        'schedule': 5.0,  # 5초마다
        'args': (16, 16)
    },
    'add-every-30-seconds': {
        'task': 'tasks.add',
        'schedule': 30.0, # 30초마다
        'args': (16, 16)
    },
}
```

## Celery monitor Tool
*Flower*
`pip install flower`  

- 작업 프로세스 상태
- 작업 프로세스 종료 및 다시시작
- 디테일한 API 관리
- Celery Monitoring

명령어  
`celery -A {file name} flower --address=localhost --port=5555`